---
name: Reusable workflow to build binary packages into S3 bucket

on:
  workflow_call:
    inputs:
      version:
        description: The version of Fluent Bit to build as a tag, commit, SHA.
        type: string
        required: true
      build_matrix:
        description: The build targets to produce as a JSON matrix.
        type: string
        required: true
      environment:
        description: The Github environment to run this workflow on.
        type: string
        required: false
      branch:
        description: The optional source code branch to use for building against.
        type: string
        required: false
      unstable:
        description: Optionally add metadata to build to indicate an unstable build, set to the contents you want to add.
        type: string
        required: false
        default: ''
    secrets:
      token:
        description: The Github token or similar to authenticate with.
        required: true
      bucket:
        description: The name of the S3 (US-East) bucket to push packages into.
        required: false
      access_key_id:
        description: The S3 access key id for the bucket.
        required: false
      secret_access_key:
        description: The S3 secret access key for the bucket.
        required: false
      gpg_private_key:
        description: The GPG key to use for signing the packages.
        required: false
      gpg_private_key_passphrase:
        description: The GPG key passphrase to use for signing the packages.
        required: false

jobs:
  call-build-packages:
    name: ${{ matrix.distro }} package build and stage to S3
    environment: ${{ inputs.environment }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(inputs.build_matrix) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ inputs.version }}

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1

      - uses: frabert/replace-string-action@master
        id: formatted_distro
        with:
          pattern: '(.*)\/(.*)$'
          string: "${{ matrix.distro }}"
          replace-with: '$1-$2'
          flags: 'g'

      - name: fluent-bit - ${{ matrix.distro }} artifacts
        run: |
          ./build.sh
        env:
          FLB_VERSION: ${{ inputs.version }}
          FLB_DISTRO: ${{ matrix.distro }}
          FLB_OUT_DIR: staging
          FLB_BRANCH: ${{ inputs.branch }}
          FLB_TD: "Off"
          FLB_NIGHTLY_BUILD: ${{ inputs.unstable }}
          CMAKE_INSTALL_PREFIX: /opt/fluent-bit/
        working-directory: packaging

      - name: td-agent-bit - ${{ matrix.distro }} artifacts
        run: |
          ./build.sh
        env:
          FLB_VERSION: ${{ inputs.version }}
          FLB_DISTRO: ${{ matrix.distro }}
          FLB_OUT_DIR: staging
          FLB_BRANCH: ${{ inputs.branch }}
          FLB_TD: "On"
          FLB_NIGHTLY_BUILD: ${{ inputs.unstable }}
          CMAKE_INSTALL_PREFIX: /opt/td-agent-bit/
        working-directory: packaging

      - name: Upload the ${{ matrix.distro }} artifacts
        uses: actions/upload-artifact@v2
        with:
          name: packages-${{ inputs.version }}-${{ steps.formatted_distro.outputs.replaced }}
          path: packaging/packages/
          if-no-files-found: error

      - name: Retrieve target info for repo creation
        id: get-target-info
        # Remove any .arm648 suffix
        # For ubuntu map to codename using the disto-info list (CSV)
        run: |
          sudo apt-get update
          sudo apt-get install -y distro-info
          TARGET=${DISTRO%*.arm64v8}
          if [[ "$TARGET" == "ubuntu/"* ]]; then
              UBUNTU_CODENAME=$(cut -d ',' -f 1,3 < "/usr/share/distro-info/ubuntu.csv"|grep "${TARGET##*/}"|cut -d ',' -f 2)
              if [[ -n "$UBUNTU_CODENAME" ]]; then
                  TARGET="ubuntu/$UBUNTU_CODENAME"
              else
                  echo "Unable to extract codename for $DISTRO"
                  exit 1
              fi
          fi
          echo "$TARGET"
          echo ::set-output name=target::$TARGET
        env:
          DISTRO: ${{ matrix.distro }}
        shell: bash

      - name: Push packages to S3
        # Only upload for staging
        if: ${{ inputs.environment == 'staging' }}
        # Make sure not to do a --delete on sync as it will remove the other architecture
        run: |
          if [ -z "${{ steps.get-target-info.outputs.target }}" ]; then
            echo "Invalid (empty) target defined"
            exit 1
          fi
          if [ -n "${AWS_S3_ENDPOINT}" ]; then
            ENDPOINT="--endpoint-url ${AWS_S3_ENDPOINT}"
          fi
          aws --region "$AWS_REGION" s3 sync "${SOURCE_DIR}" "s3://${AWS_S3_BUCKET}/${DEST_DIR}" --follow-symlinks --no-progress ${ENDPOINT}
        env:
          SOURCE_DIR: "packaging/packages/${{ matrix.distro }}/${{ inputs.version }}/staging/"
          DEST_DIR: "${{ inputs.version }}/${{ steps.get-target-info.outputs.target }}/"
          AWS_REGION: "us-east-1"
          AWS_ACCESS_KEY_ID: ${{ secrets.access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.secret_access_key }}
          AWS_S3_BUCKET: ${{ secrets.bucket }}

  call-build-packages-repo:
    name: Create repo metadata in S3
    # Only upload for staging
    if: ${{ inputs.environment == 'staging' }}
    # Need to use 18.04 as 20.04 has no createrepo available
    runs-on: ubuntu-18.04
    environment: ${{ inputs.environment }}
    needs: call-build-packages
    steps:
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y createrepo aptly

      - name: Checkout code
        uses: actions/checkout@v3

      - name: Import GPG key for signing
        id: import_gpg
        uses: crazy-max/ghaction-import-gpg@v4
        with:
          gpg_private_key: ${{ secrets.gpg_private_key }}
          passphrase: ${{ secrets.gpg_private_key_passphrase }}

      - name: Create repositories on staging now
        # We sync down what we have for the release directories.
        # Create the repo metadata then upload to the root of the bucket.
        # This will wipe out any versioned directories in the process.
        run: |
          rm -rf ./latest/
          mkdir -p ./latest/
          if [ -n "${AWS_S3_ENDPOINT}" ]; then
            ENDPOINT="--endpoint-url ${AWS_S3_ENDPOINT}"
          fi
          aws s3 sync "s3://$AWS_S3_BUCKET/${{ inputs.version }}" ./latest/ --no-progress ${ENDPOINT}

          gpg --export -a "${{ steps.import_gpg.outputs.name }}" > ./latest/fluentbit.key
          rpm --import ./latest/fluentbit.key

          ./update-repos.sh "${{ inputs.version }}" "./latest/"
          echo "${{ inputs.version }}" > "./latest/latest-version.txt"
          aws s3 sync "./latest/" "s3://$AWS_S3_BUCKET" --delete --follow-symlinks --no-progress ${ENDPOINT}
        env:
          GPG_KEY: ${{ steps.import_gpg.outputs.name }}
          AWS_REGION: "us-east-1"
          AWS_ACCESS_KEY_ID: ${{ secrets.access_key_id }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.secret_access_key }}
          AWS_S3_BUCKET: ${{ secrets.bucket }}
          # To use with Minio locally (or update to whatever endpoint you want)
          # AWS_S3_ENDPOINT: http://localhost:9000
        shell: bash
        working-directory: packaging
